{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11938985,"sourceType":"datasetVersion","datasetId":7506005}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nEnhanced Bundle Suggester with BERT Embeddings and FAISS Search\nCombines Qwen-generated suggestions with semantic similarity search using BERT and FAISS.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport logging\nimport pickle\nimport os\nimport sys\nfrom itertools import islice\nfrom typing import List, Dict, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check and install required packages\ndef install_if_missing(package_name, import_name=None):\n    if import_name is None:\n        import_name = package_name\n    \n    try:\n        __import__(import_name)\n    except ImportError:\n        print(f\"{package_name} not found. Installing...\")\n        os.system(f\"pip install {package_name}\")\n\n# Install required packages\ninstall_if_missing(\"sentence-transformers\")\ninstall_if_missing(\"faiss-cpu\", \"faiss\")\ninstall_if_missing(\"vllm\")\ninstall_if_missing(\"huggingface_hub\")\ninstall_if_missing(\"torch\")\ninstall_if_missing(\"transformers\")\n\n# Now import everything\nimport torch\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nfrom huggingface_hub import snapshot_download\nimport vllm\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef filt(text: str, max_items: int = 10) -> str:\n    \"\"\"Extract and clean numbered list items from text.\"\"\"\n    items = re.findall(r'^\\s*\\d+\\.\\s*(.+)$', text, re.MULTILINE)\n    items = items[:max_items]\n    cleaned_items = [re.sub(r'\\s*\\(.*?\\)', '', item).strip() for item in items]\n    return ' | '.join(cleaned_items)\n\nclass BERTEmbedder:\n    \"\"\"Handles BERT multilingual embeddings for product titles.\"\"\"\n    \n    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n        \"\"\"\n        Initialize BERT embedder with multilingual model.\n        \n        Args:\n            model_name: Sentence transformer model name\n        \"\"\"\n        print(f\"Loading BERT model: {model_name}\")\n        self.model = SentenceTransformer(model_name)\n        self.embeddings = None\n        self.item_titles = None\n        print(\"BERT model loaded successfully\")\n    \n    def encode_items(self, item_titles: List[str], batch_size: int = 32) -> np.ndarray:\n        \"\"\"\n        Encode item titles to embeddings.\n        \n        Args:\n            item_titles: List of product titles\n            batch_size: Batch size for encoding\n            \n        Returns:\n            Array of embeddings\n        \"\"\"\n        print(f\"Encoding {len(item_titles)} items with BERT...\")\n        self.item_titles = item_titles\n        \n        # Encode in batches to manage memory\n        embeddings = self.model.encode(\n            item_titles,\n            batch_size=batch_size,\n            show_progress_bar=True,\n            convert_to_numpy=True\n        )\n        \n        self.embeddings = embeddings\n        print(f\"Generated embeddings shape: {embeddings.shape}\")\n        return embeddings\n    \n    def save_embeddings(self, filepath: str):\n        \"\"\"Save embeddings and titles to file.\"\"\"\n        data = {\n            'embeddings': self.embeddings,\n            'item_titles': self.item_titles\n        }\n        with open(filepath, 'wb') as f:\n            pickle.dump(data, f)\n        print(f\"Embeddings saved to {filepath}\")\n    \n    def load_embeddings(self, filepath: str):\n        \"\"\"Load embeddings and titles from file.\"\"\"\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n        self.embeddings = data['embeddings']\n        self.item_titles = data['item_titles']\n        print(f\"Embeddings loaded from {filepath}\")\n\nclass FAISSSearcher:\n    \"\"\"FAISS-based similarity search for product embeddings.\"\"\"\n    \n    def __init__(self, embeddings: np.ndarray, item_titles: List[str]):\n        \"\"\"\n        Initialize FAISS index with embeddings.\n        \n        Args:\n            embeddings: Numpy array of embeddings\n            item_titles: List of corresponding item titles\n        \"\"\"\n        self.embeddings = embeddings\n        self.item_titles = item_titles\n        self.dimension = embeddings.shape[1]\n        \n        # Create FAISS index\n        print(\"Building FAISS index...\")\n        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product for cosine similarity\n        \n        # Normalize embeddings for cosine similarity\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype('float32'))\n        \n        print(f\"FAISS index built with {self.index.ntotal} vectors\")\n    \n    def search_similar_items(self, query_text: str, bert_model: SentenceTransformer, \n                           top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Search for similar items given a query text.\n        \n        Args:\n            query_text: Text to search for\n            bert_model: BERT model for encoding query\n            top_k: Number of top results to return\n            \n        Returns:\n            List of (item_title, similarity_score) tuples\n        \"\"\"\n        # Encode query\n        query_embedding = bert_model.encode([query_text], convert_to_numpy=True)\n        faiss.normalize_L2(query_embedding)\n        \n        # Search\n        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n        \n        # Return results\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.item_titles):  # Valid index\n                results.append((self.item_titles[idx], float(score)))\n        \n        return results\n    \n    def find_complementary_items(self, complement_categories: str, bert_model: SentenceTransformer,\n                               original_item: str = None, top_k: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"\n        Find items that match complementary categories.\n        \n        Args:\n            complement_categories: Pipe-separated categories from Qwen\n            bert_model: BERT model for encoding\n            original_item: Original item to exclude from results\n            top_k: Number of results per category\n            \n        Returns:\n            List of (item_title, similarity_score) tuples\n        \"\"\"\n        if not complement_categories or complement_categories == \"\":\n            return []\n        \n        categories = [cat.strip() for cat in complement_categories.split('|')]\n        all_results = []\n        \n        for category in categories:\n            if category:\n                results = self.search_similar_items(category, bert_model, top_k)\n                # Filter out the original item if specified\n                if original_item:\n                    results = [(item, score) for item, score in results \n                             if item.lower() != original_item.lower()]\n                all_results.extend(results)\n        \n        # Remove duplicates and sort by score\n        seen = set()\n        unique_results = []\n        for item, score in all_results:\n            if item not in seen:\n                seen.add(item)\n                unique_results.append((item, score))\n        \n        # Sort by similarity score (descending)\n        unique_results.sort(key=lambda x: x[1], reverse=True)\n        return unique_results[:top_k * 2]  # Return more results\n\n\n    def format_prompt(self, description: str) -> str:\n        return self.prompt_template.format(description=description)\n\n    def suggest_for_batch(self, descriptions: List[str]) -> List[str]:\n        prompts = [self.format_prompt(d) for d in descriptions]\n        try:\n            responses = self.llm.generate(prompts, self.sampling_params, use_tqdm=False)\n            outputs = []\n            for resp in responses:\n                text = resp.outputs[0].text.strip()\n                outputs.append(filt(text))\n            return outputs\n        except Exception as e:\n            logging.error(\"Error in batch generation: %s\", e)\n            return [\"- Error generating suggestions.\"] * len(descriptions)\n\ndef batch_iterator(iterable, size: int = 32):\n    \"\"\"Yield successive batches from iterable.\"\"\"\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, size))\n        if not chunk:\n            break\n        yield chunk\n\n    df = pd.DataFrame(sample_data)\n    df.to_csv('sample_inventory.csv', index=False)\n    print(\"Created sample_inventory.csv for testing\")\n    return 'sample_inventory.csv'\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    # Configuration\n    INPUT_CSV = \"/kaggle/input/datasettt/final_dataset.csv\"\n    OUTPUT_CSV = \"final_dataset_with_suggestions.csv\"\n    EMBEDDINGS_FILE = \"bert_embeddings.pkl\"\n    BATCH_SIZE = 32\n    \n    # Check if input file exists\n    if not os.path.exists(INPUT_CSV):\n        print(f\"Input file {INPUT_CSV} not found.\")\n        choice = input(\"Create sample data for testing? (y/n): \").lower()\n        if choice == 'y':\n            INPUT_CSV = create_sample_data()\n        else:\n            print(\"Please provide a valid CSV file with 'Item title' column.\")\n            sys.exit(1)\n    \n    # Load data\n    print(\"Loading data...\")\n    try:\n        df = pd.read_csv(INPUT_CSV)\n        if 'Item title' not in df.columns:\n            print(\"Error: CSV must contain 'Item title' column\")\n            sys.exit(1)\n    except Exception as e:\n        print(f\"Error loading CSV: {e}\")\n        sys.exit(1)\n    \n    items = df[\"Item title\"].tolist()\n    print(f\"Loaded {len(items)} items from {INPUT_CSV}\")\n    \n    # Step 1: Generate embeddings with BERT\n    print(\"\\n=== Step 1: Generating BERT Embeddings ===\")\n    bert_embedder = BERTEmbedder()\n    \n    if os.path.exists(EMBEDDINGS_FILE):\n        print(\"Loading existing embeddings...\")\n        bert_embedder.load_embeddings(EMBEDDINGS_FILE)\n    else:\n        embeddings = bert_embedder.encode_items(items)\n        bert_embedder.save_embeddings(EMBEDDINGS_FILE)\n    \n    # Step 2: Create FAISS index\n    print(\"\\n=== Step 2: Building FAISS Index ===\")\n    faiss_searcher = FAISSSearcher(bert_embedder.embeddings, bert_embedder.item_titles)\n    \n    # Step 3: Generate complementary categories (if not already present)\n    if 'compl_items' not in df.columns:\n        print(\"\\n=== No compl items found ===\")\n    else:\n        print(\"Using existing complementary categories...\")\n    \n    # Step 4: Find similar items using FAISS\n    print(\"\\n=== Step 4: Finding Similar Items with FAISS ===\")\n    similar_items_list = []\n    similarity_scores_list = []\n    \n    for idx, row in df.iterrows():\n        original_item = row['Item title']\n        complement_categories = row.get('compl_items', '')\n        \n        if pd.isna(complement_categories) or complement_categories == '':\n            similar_items_list.append('')\n            similarity_scores_list.append('')\n            continue\n        \n        # Find complementary items\n        similar_results = faiss_searcher.find_complementary_items(\n            complement_categories, \n            bert_embedder.model,\n            original_item=original_item,\n            top_k=3\n        )\n        \n        # Format results\n        if similar_results:\n            items_str = ' | '.join([item for item, _ in similar_results[:5]])\n            scores_str = ' | '.join([f\"{score:.3f}\" for _, score in similar_results[:5]])\n        else:\n            items_str = ''\n            scores_str = ''\n        \n        similar_items_list.append(items_str)\n        similarity_scores_list.append(scores_str)\n        \n        if (idx + 1) % 100 == 0:\n            print(f\"Processed {idx + 1}/{len(df)} items\")\n    \n    # Add results to dataframe\n    df['similar_items'] = similar_items_list\n    df['similarity_scores'] = similarity_scores_list\n    \n    # Step 5: Save results\n    print(\"\\n=== Step 5: Saving Results ===\")\n    df.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Results saved to {OUTPUT_CSV}\")\n    \n    # Show sample results\n    print(\"\\n=== Sample Results ===\")\n    for i in range(min(3, len(df))):\n        print(f\"\\nItem #{i+1}:\")\n        print(f\"Original: {df.iloc[i]['Item title']}\")\n        print(f\"Categories: {df.iloc[i]['compl_items']}\")\n        print(f\"Similar Items: {df.iloc[i]['similar_items']}\")\n        print(f\"Scores: {df.iloc[i]['similarity_scores']}\")\n        print(\"-\" * 80)\n    \n    print(f\"\\nProcessing complete! Check {OUTPUT_CSV} for full results.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}