{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11938653,"sourceType":"datasetVersion","datasetId":7505166},{"sourceId":166355,"sourceType":"modelInstanceVersion","modelInstanceId":141552,"modelId":164048},{"sourceId":166361,"sourceType":"modelInstanceVersion","modelInstanceId":141558,"modelId":164048}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths to your CSV files\norders_file_path = '/kaggle/input/datasett/orders_data.csv'\ninventory_file_path = '/kaggle/input/datasett/inventory_data.csv'\n\ntry:\n    # Load the Orders data\n    df_orders = pd.read_csv(orders_file_path)\n    print(\"Successfully loaded orders.csv\")\n    print(\"First 5 rows of the Orders DataFrame:\")\n    print(df_orders.head())\n    print(\"\\nOrders DataFrame Info:\")\n    df_orders.info()\n    print(\"-\" * 50)\n\n    # Load the Inventory data\n    df_inventory = pd.read_csv(inventory_file_path)\n    print(\"\\nSuccessfully loaded inventory.csv\")\n    print(\"First 5 rows of the Inventory DataFrame:\")\n    print(df_inventory.head())\n    print(\"\\nInventory DataFrame Info:\")\n    df_inventory.info()\n    print(\"-\" * 50)\n\n    # Now you can proceed with the data augmentation steps\n    # as discussed previously. For example:\n\n    # 1. Extract unique item information from orders\n    if not df_orders.empty:\n        item_details = df_orders[['SKU', 'Item title', 'Category', 'Brand']].drop_duplicates(subset=['SKU'])\n\n        # 2. (Optional) Augment with pricing information\n        item_pricing = df_orders[['SKU', 'OriginalUnitPrice']].drop_duplicates(subset=['SKU']) # Or use 'Final UnitPrice'\n\n        # Merge item details with pricing\n        item_info_for_inventory = pd.merge(item_details, item_pricing, on='SKU', how='left')\n\n        # 3. Merge this extracted information into the inventory dataset\n        if not df_inventory.empty:\n            df_inventory_augmented = pd.merge(df_inventory, item_info_for_inventory, on='SKU', how='left')\n            print(\"\\nAugmented Inventory Data (first 5 rows):\")\n            print(df_inventory_augmented.head())\n            # Save the augmented inventory DataFrame to a CSV file\n            output_file_path = '/kaggle/working/inventory_augmented.csv'  # Adjust this path as needed\n            \n            try:\n                df_inventory_augmented.to_csv(output_file_path, index=False)\n                print(f\"\\nAugmented Inventory DataFrame saved to {output_file_path}\")\n            except Exception as e:\n                print(f\"An error occurred while saving the file: {e}\")\n        else:\n            print(\"\\nInventory DataFrame is empty. Augmentation skipped.\")\n            df_inventory_augmented = pd.DataFrame() # Create an empty df\n    else:\n        print(\"\\nOrders DataFrame is empty. Augmentation skipped.\")\n        df_inventory_augmented = df_inventory.copy() # Use inventory as is or an empty df if inventory is also empty\n\n\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}. Please make sure the CSV files are in the correct directory.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T23:43:14.284450Z","iopub.execute_input":"2025-05-24T23:43:14.284773Z","iopub.status.idle":"2025-05-24T23:43:14.289651Z","shell.execute_reply.started":"2025-05-24T23:43:14.284753Z","shell.execute_reply":"2025-05-24T23:43:14.288978Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T23:43:14.290407Z","iopub.execute_input":"2025-05-24T23:43:14.290650Z","iopub.status.idle":"2025-05-24T23:43:14.316594Z","shell.execute_reply.started":"2025-05-24T23:43:14.290626Z","shell.execute_reply":"2025-05-24T23:43:14.316055Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import vllm\nimport kagglehub\nimport logging\n\nnum_attempt = 1\n\nimport re\n\ndef filt(text, max_items=10):\n    \"\"\"\n    Extracts up to `max_items` numbered list items from the input text, removes any parenthetical content,\n    and returns them as a single string separated by ' | '.\n    \"\"\"\n    # Find all items\n    items = re.findall(r'^\\s*\\d+\\.\\s*(.+)$', text, re.MULTILINE)\n    # Take only the first max_items\n    items = items[:max_items]\n    # Remove parenthetical content from each item\n    cleaned_items = [re.sub(r'\\s*\\(.*?\\)', '', item).strip() for item in items]\n    return ' | '.join(cleaned_items)\n\nclass BundleSuggester:\n    def __init__(self, tensor_parallel_size=2, gpu_memory_utilization=0.95):\n        # Download and load the model once\n        self.model_path = kagglehub.model_download(\n            'qwen-lm/qwen2.5/Transformers/14b-instruct-awq/1'\n        )\n        self.llm = vllm.LLM(\n            self.model_path,\n            quantization=\"awq\",\n            tensor_parallel_size=tensor_parallel_size,\n            gpu_memory_utilization=gpu_memory_utilization,\n            trust_remote_code=True,\n            dtype=\"half\",\n            enforce_eager=True,\n            max_model_len=2048,\n            disable_log_stats=True\n        )\n        self.sampling_params = vllm.SamplingParams(\n            n=1,\n            top_k=20,\n            top_p=0.8,\n            temperature=0.7,\n            repetition_penalty=1.05,\n            skip_special_tokens=False,\n            max_tokens=128,\n        )\n\n        # Prompt template\n        self.prompt_template = \"\"\"\nYou are an e-commerce bundling expert. Generate product bundle suggestions for the given item.\n\nConsider these bundle strategies:\n- Complementary items (frequently bought together)\n- Thematic collections (occasion/lifestyle based)\n- Do not recommend items that are the exact same type (do not recommend another lipstick if the item is a lipstick).\n- Take into account the gender that is most likely to prefer the given product during recommendations.\n- Do not recommend the same category more than one time.\n\nRequirements:\n- List exactly 10 generic product categories.\n- Use simple category names no brands, models, or specifications (example: T-shirt, mascara, jeans, bras, etc).\n- Rank by bundling potential and revenue impact.\n- Output format: numbered list only\n\nInput: {description}\n\nOutput format:\n1. [Category Name]\n2. [Category Name]\n3. [Category Name]\n...\n\"\"\"\n    def format_prompt(self, description: str) -> str:\n        return self.prompt_template.format(description=description)\n\n    def suggest_for_batch(self, descriptions: list[str]) -> list[str]:\n        prompts = [self.format_prompt(d) for d in descriptions]\n        # Batch generate\n        try:\n            responses = self.llm.generate(prompts, self.sampling_params, use_tqdm=False)\n            outputs = []\n            for resp in responses:\n                text = resp.outputs[0].text.strip()\n                outputs.append(filt(text))\n            return outputs\n        except Exception as e:\n            logging.error(\"Error in batch generation: %s\", e)\n            return [\"- Error generating suggestions.\"] * len(descriptions)\n\nfrom itertools import islice\n\ndef batch_iterator(iterable, size=32):\n    it = iter(iterable)\n    while True:\n        chunk = list(islice(it, size))\n        if not chunk:\n            break\n        yield chunk\n\n\n#model = BundleSuggester()\n#out = model.predict('Hugo Boss ανδρικό μαγιό σορτς με all-over handwritten logo \"Marco\" Μπλε Σκούρο L')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T23:43:14.364184Z","iopub.execute_input":"2025-05-24T23:43:14.364682Z","iopub.status.idle":"2025-05-24T23:43:28.001911Z","shell.execute_reply.started":"2025-05-24T23:43:14.364663Z","shell.execute_reply":"2025-05-24T23:43:28.001205Z"}},"outputs":[{"name":"stdout","text":"INFO 05-24 23:43:20 [__init__.py:239] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-05-24 23:43:21.283546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748130201.314825     310 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748130201.322504     310 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# out = model.predict('Hugo Boss ανδρικό μαγιό σορτς με all-over handwritten logo \"Marco\" Μπλε Σκούρο L')\n# print(out)\n# print(filt(out))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T23:43:28.003396Z","iopub.execute_input":"2025-05-24T23:43:28.003683Z","iopub.status.idle":"2025-05-24T23:43:28.007431Z","shell.execute_reply.started":"2025-05-24T23:43:28.003655Z","shell.execute_reply":"2025-05-24T23:43:28.006623Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/datasett/inventory_augmented.csv\")\n\n# total = len(df)\n# print(f\"Starting processing {total} rows...\\n\")\n\n# for idx, title in enumerate(df[\"Item title\"], start=1):\n#     compl_items.append(filt(model.predict(title)))\n    \n#     # Print progress every 100 rows (or on last row)\n#     if idx % 10 == 0 or idx == total:\n#         print(f\"Processed {idx}/{total} rows\")\n\n# # Assign the new column\n# df[\"compl_items\"] = compl_items\n\n# # Save to CSV\n# df.to_csv('final_dataset.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T23:43:28.008248Z","iopub.execute_input":"2025-05-24T23:43:28.008549Z","iopub.status.idle":"2025-05-24T23:43:28.027550Z","shell.execute_reply.started":"2025-05-24T23:43:28.008521Z","shell.execute_reply":"2025-05-24T23:43:28.026908Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# Configuration\nINPUT_CSV = \"/kaggle/input/datasett/inventory_augmented.csv\"\nOUTPUT_CSV = \"final_dataset.csv\"\nBATCH_SIZE = 32  # adjust based on VRAM\n\n# Load data\nprint(\"Loading data...\")\ndf = pd.read_csv(INPUT_CSV)\nitems = df[\"Item title\"].tolist()\n\n# Initialize suggester\nsuggester = BundleSuggester()\n\n# Run in batches\nall_suggestions = []\nprint(f\"Processing {len(items)} items in batches of {BATCH_SIZE}...\")\nfor idx, batch in enumerate(batch_iterator(items, BATCH_SIZE), start=1):\n    batch_suggestions = suggester.suggest_for_batch(batch)\n    all_suggestions.extend(batch_suggestions)\n    print(f\"Batch {idx}: processed {len(batch)} items (total {len(all_suggestions)}/{len(items)})\")\n\n# Attach to dataframe and save\nprint(\"Assigning suggestions and saving...\")\ndf[\"compl_items\"] = all_suggestions\ndf.to_csv(OUTPUT_CSV, index=False)\nprint(\"Done. Output saved to\", OUTPUT_CSV)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T23:43:28.028268Z","iopub.execute_input":"2025-05-24T23:43:28.028515Z","iopub.status.idle":"2025-05-25T00:55:05.464648Z","shell.execute_reply.started":"2025-05-24T23:43:28.028490Z","shell.execute_reply":"2025-05-25T00:55:05.463959Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nINFO 05-24 23:43:42 [config.py:717] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\nWARNING 05-24 23:43:43 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nWARNING 05-24 23:43:43 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nINFO 05-24 23:43:43 [config.py:1770] Defaulting to use mp for distributed inference\nWARNING 05-24 23:43:43 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 05-24 23:43:43 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \nWARNING 05-24 23:43:44 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\nWARNING 05-24 23:43:44 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 05-24 23:43:44 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 05-24 23:43:44 [cuda.py:289] Using XFormers backend.\nINFO 05-24 23:43:49 [__init__.py:239] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-05-24 23:43:49.840839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748130229.862449     353 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748130229.869120     353 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:43:55 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:43:56 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:43:56 [cuda.py:289] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"[W524 23:44:07.009617326 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W524 23:44:07.393421489 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W524 23:44:17.015871593 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 05-24 23:44:27 [utils.py:1055] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:27 [utils.py:1055] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:27 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 05-24 23:44:27 [pynccl.py:69] vLLM is using nccl==2.21.5\n","output_type":"stream"},{"name":"stderr","text":"[W524 23:44:27.026393692 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 05-24 23:44:27 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:27 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m WARNING 05-24 23:44:27 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 05-24 23:44:27 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 05-24 23:44:27 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_bdd4c5d1'), local_subscribe_addr='ipc:///tmp/3b885bec-793e-48d9-ba94-f3dc7c856061', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 05-24 23:44:27 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:27 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\nINFO 05-24 23:44:27 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:27 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac13d9225df24145a5dcc5a5f2baf784"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:39 [loader.py:458] Loading weights took 11.25 seconds\nINFO 05-24 23:44:39 [loader.py:458] Loading weights took 11.29 seconds\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:39 [model_runner.py:1140] Model loading took 4.6720 GiB and 11.483941 seconds\nINFO 05-24 23:44:39 [model_runner.py:1140] Model loading took 4.6720 GiB and 11.532345 seconds\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:44 [worker.py:287] Memory profiling takes 4.45 seconds\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:44 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\n\u001b[1;36m(VllmWorkerProcess pid=353)\u001b[0;0m INFO 05-24 23:44:44 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 0.25GiB; the rest of the memory reserved for KV Cache is 8.98GiB.\nINFO 05-24 23:44:44 [worker.py:287] Memory profiling takes 4.57 seconds\nINFO 05-24 23:44:44 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\nINFO 05-24 23:44:44 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 7.82GiB.\nINFO 05-24 23:44:45 [executor_base.py:112] # cuda blocks: 5339, # CPU blocks: 2730\nINFO 05-24 23:44:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 41.71x\nINFO 05-24 23:44:50 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 10.37 seconds\nProcessing 6825 items in batches of 32...\nBatch 1: processed 32 items (total 32/6825)\nBatch 2: processed 32 items (total 64/6825)\nBatch 3: processed 32 items (total 96/6825)\nBatch 4: processed 32 items (total 128/6825)\nBatch 5: processed 32 items (total 160/6825)\nBatch 6: processed 32 items (total 192/6825)\nBatch 7: processed 32 items (total 224/6825)\nBatch 8: processed 32 items (total 256/6825)\nBatch 9: processed 32 items (total 288/6825)\nBatch 10: processed 32 items (total 320/6825)\nBatch 11: processed 32 items (total 352/6825)\nBatch 12: processed 32 items (total 384/6825)\nBatch 13: processed 32 items (total 416/6825)\nBatch 14: processed 32 items (total 448/6825)\nBatch 15: processed 32 items (total 480/6825)\nBatch 16: processed 32 items (total 512/6825)\nBatch 17: processed 32 items (total 544/6825)\nBatch 18: processed 32 items (total 576/6825)\nBatch 19: processed 32 items (total 608/6825)\nBatch 20: processed 32 items (total 640/6825)\nBatch 21: processed 32 items (total 672/6825)\nBatch 22: processed 32 items (total 704/6825)\nBatch 23: processed 32 items (total 736/6825)\nBatch 24: processed 32 items (total 768/6825)\nBatch 25: processed 32 items (total 800/6825)\nBatch 26: processed 32 items (total 832/6825)\nBatch 27: processed 32 items (total 864/6825)\nBatch 28: processed 32 items (total 896/6825)\nBatch 29: processed 32 items (total 928/6825)\nBatch 30: processed 32 items (total 960/6825)\nBatch 31: processed 32 items (total 992/6825)\nBatch 32: processed 32 items (total 1024/6825)\nBatch 33: processed 32 items (total 1056/6825)\nBatch 34: processed 32 items (total 1088/6825)\nBatch 35: processed 32 items (total 1120/6825)\nBatch 36: processed 32 items (total 1152/6825)\nBatch 37: processed 32 items (total 1184/6825)\nBatch 38: processed 32 items (total 1216/6825)\nBatch 39: processed 32 items (total 1248/6825)\nBatch 40: processed 32 items (total 1280/6825)\nBatch 41: processed 32 items (total 1312/6825)\nBatch 42: processed 32 items (total 1344/6825)\nBatch 43: processed 32 items (total 1376/6825)\nBatch 44: processed 32 items (total 1408/6825)\nBatch 45: processed 32 items (total 1440/6825)\nBatch 46: processed 32 items (total 1472/6825)\nBatch 47: processed 32 items (total 1504/6825)\nBatch 48: processed 32 items (total 1536/6825)\nBatch 49: processed 32 items (total 1568/6825)\nBatch 50: processed 32 items (total 1600/6825)\nBatch 51: processed 32 items (total 1632/6825)\nBatch 52: processed 32 items (total 1664/6825)\nBatch 53: processed 32 items (total 1696/6825)\nBatch 54: processed 32 items (total 1728/6825)\nBatch 55: processed 32 items (total 1760/6825)\nBatch 56: processed 32 items (total 1792/6825)\nBatch 57: processed 32 items (total 1824/6825)\nBatch 58: processed 32 items (total 1856/6825)\nBatch 59: processed 32 items (total 1888/6825)\nBatch 60: processed 32 items (total 1920/6825)\nBatch 61: processed 32 items (total 1952/6825)\nBatch 62: processed 32 items (total 1984/6825)\nBatch 63: processed 32 items (total 2016/6825)\nBatch 64: processed 32 items (total 2048/6825)\nBatch 65: processed 32 items (total 2080/6825)\nBatch 66: processed 32 items (total 2112/6825)\nBatch 67: processed 32 items (total 2144/6825)\nBatch 68: processed 32 items (total 2176/6825)\nBatch 69: processed 32 items (total 2208/6825)\nBatch 70: processed 32 items (total 2240/6825)\nBatch 71: processed 32 items (total 2272/6825)\nBatch 72: processed 32 items (total 2304/6825)\nBatch 73: processed 32 items (total 2336/6825)\nBatch 74: processed 32 items (total 2368/6825)\nBatch 75: processed 32 items (total 2400/6825)\nBatch 76: processed 32 items (total 2432/6825)\nBatch 77: processed 32 items (total 2464/6825)\nBatch 78: processed 32 items (total 2496/6825)\nBatch 79: processed 32 items (total 2528/6825)\nBatch 80: processed 32 items (total 2560/6825)\nBatch 81: processed 32 items (total 2592/6825)\nBatch 82: processed 32 items (total 2624/6825)\nBatch 83: processed 32 items (total 2656/6825)\nBatch 84: processed 32 items (total 2688/6825)\nBatch 85: processed 32 items (total 2720/6825)\nBatch 86: processed 32 items (total 2752/6825)\nBatch 87: processed 32 items (total 2784/6825)\nBatch 88: processed 32 items (total 2816/6825)\nBatch 89: processed 32 items (total 2848/6825)\nBatch 90: processed 32 items (total 2880/6825)\nBatch 91: processed 32 items (total 2912/6825)\nBatch 92: processed 32 items (total 2944/6825)\nBatch 93: processed 32 items (total 2976/6825)\nBatch 94: processed 32 items (total 3008/6825)\nBatch 95: processed 32 items (total 3040/6825)\nBatch 96: processed 32 items (total 3072/6825)\nBatch 97: processed 32 items (total 3104/6825)\nBatch 98: processed 32 items (total 3136/6825)\nBatch 99: processed 32 items (total 3168/6825)\nBatch 100: processed 32 items (total 3200/6825)\nBatch 101: processed 32 items (total 3232/6825)\nBatch 102: processed 32 items (total 3264/6825)\nBatch 103: processed 32 items (total 3296/6825)\nBatch 104: processed 32 items (total 3328/6825)\nBatch 105: processed 32 items (total 3360/6825)\nBatch 106: processed 32 items (total 3392/6825)\nBatch 107: processed 32 items (total 3424/6825)\nBatch 108: processed 32 items (total 3456/6825)\nBatch 109: processed 32 items (total 3488/6825)\nBatch 110: processed 32 items (total 3520/6825)\nBatch 111: processed 32 items (total 3552/6825)\nBatch 112: processed 32 items (total 3584/6825)\nBatch 113: processed 32 items (total 3616/6825)\nBatch 114: processed 32 items (total 3648/6825)\nBatch 115: processed 32 items (total 3680/6825)\nBatch 116: processed 32 items (total 3712/6825)\nBatch 117: processed 32 items (total 3744/6825)\nBatch 118: processed 32 items (total 3776/6825)\nBatch 119: processed 32 items (total 3808/6825)\nBatch 120: processed 32 items (total 3840/6825)\nBatch 121: processed 32 items (total 3872/6825)\nBatch 122: processed 32 items (total 3904/6825)\nBatch 123: processed 32 items (total 3936/6825)\nBatch 124: processed 32 items (total 3968/6825)\nBatch 125: processed 32 items (total 4000/6825)\nBatch 126: processed 32 items (total 4032/6825)\nBatch 127: processed 32 items (total 4064/6825)\nBatch 128: processed 32 items (total 4096/6825)\nBatch 129: processed 32 items (total 4128/6825)\nBatch 130: processed 32 items (total 4160/6825)\nBatch 131: processed 32 items (total 4192/6825)\nBatch 132: processed 32 items (total 4224/6825)\nBatch 133: processed 32 items (total 4256/6825)\nBatch 134: processed 32 items (total 4288/6825)\nBatch 135: processed 32 items (total 4320/6825)\nBatch 136: processed 32 items (total 4352/6825)\nBatch 137: processed 32 items (total 4384/6825)\nBatch 138: processed 32 items (total 4416/6825)\nBatch 139: processed 32 items (total 4448/6825)\nBatch 140: processed 32 items (total 4480/6825)\nBatch 141: processed 32 items (total 4512/6825)\nBatch 142: processed 32 items (total 4544/6825)\nBatch 143: processed 32 items (total 4576/6825)\nBatch 144: processed 32 items (total 4608/6825)\nBatch 145: processed 32 items (total 4640/6825)\nBatch 146: processed 32 items (total 4672/6825)\nBatch 147: processed 32 items (total 4704/6825)\nBatch 148: processed 32 items (total 4736/6825)\nBatch 149: processed 32 items (total 4768/6825)\nBatch 150: processed 32 items (total 4800/6825)\nBatch 151: processed 32 items (total 4832/6825)\nBatch 152: processed 32 items (total 4864/6825)\nBatch 153: processed 32 items (total 4896/6825)\nBatch 154: processed 32 items (total 4928/6825)\nBatch 155: processed 32 items (total 4960/6825)\nBatch 156: processed 32 items (total 4992/6825)\nBatch 157: processed 32 items (total 5024/6825)\nBatch 158: processed 32 items (total 5056/6825)\nBatch 159: processed 32 items (total 5088/6825)\nBatch 160: processed 32 items (total 5120/6825)\nBatch 161: processed 32 items (total 5152/6825)\nBatch 162: processed 32 items (total 5184/6825)\nBatch 163: processed 32 items (total 5216/6825)\nBatch 164: processed 32 items (total 5248/6825)\nBatch 165: processed 32 items (total 5280/6825)\nBatch 166: processed 32 items (total 5312/6825)\nBatch 167: processed 32 items (total 5344/6825)\nBatch 168: processed 32 items (total 5376/6825)\nBatch 169: processed 32 items (total 5408/6825)\nBatch 170: processed 32 items (total 5440/6825)\nBatch 171: processed 32 items (total 5472/6825)\nBatch 172: processed 32 items (total 5504/6825)\nBatch 173: processed 32 items (total 5536/6825)\nBatch 174: processed 32 items (total 5568/6825)\nBatch 175: processed 32 items (total 5600/6825)\nBatch 176: processed 32 items (total 5632/6825)\nBatch 177: processed 32 items (total 5664/6825)\nBatch 178: processed 32 items (total 5696/6825)\nBatch 179: processed 32 items (total 5728/6825)\nBatch 180: processed 32 items (total 5760/6825)\nBatch 181: processed 32 items (total 5792/6825)\nBatch 182: processed 32 items (total 5824/6825)\nBatch 183: processed 32 items (total 5856/6825)\nBatch 184: processed 32 items (total 5888/6825)\nBatch 185: processed 32 items (total 5920/6825)\nBatch 186: processed 32 items (total 5952/6825)\nBatch 187: processed 32 items (total 5984/6825)\nBatch 188: processed 32 items (total 6016/6825)\nBatch 189: processed 32 items (total 6048/6825)\nBatch 190: processed 32 items (total 6080/6825)\nBatch 191: processed 32 items (total 6112/6825)\nBatch 192: processed 32 items (total 6144/6825)\nBatch 193: processed 32 items (total 6176/6825)\nBatch 194: processed 32 items (total 6208/6825)\nBatch 195: processed 32 items (total 6240/6825)\nBatch 196: processed 32 items (total 6272/6825)\nBatch 197: processed 32 items (total 6304/6825)\nBatch 198: processed 32 items (total 6336/6825)\nBatch 199: processed 32 items (total 6368/6825)\nBatch 200: processed 32 items (total 6400/6825)\nBatch 201: processed 32 items (total 6432/6825)\nBatch 202: processed 32 items (total 6464/6825)\nBatch 203: processed 32 items (total 6496/6825)\nBatch 204: processed 32 items (total 6528/6825)\nBatch 205: processed 32 items (total 6560/6825)\nBatch 206: processed 32 items (total 6592/6825)\nBatch 207: processed 32 items (total 6624/6825)\nBatch 208: processed 32 items (total 6656/6825)\nBatch 209: processed 32 items (total 6688/6825)\nBatch 210: processed 32 items (total 6720/6825)\nBatch 211: processed 32 items (total 6752/6825)\nBatch 212: processed 32 items (total 6784/6825)\nBatch 213: processed 32 items (total 6816/6825)\nBatch 214: processed 9 items (total 6825/6825)\nAssigning suggestions and saving...\nDone. Output saved to final_dataset.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}